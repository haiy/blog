---
title: RNN基本原理和手写实现
layout: post
---

{{page.title}}
=============

<p class="meta">22 Mar 2018</p>

Table of Content
=================
   * [前向计算](#前向计算)
   * [反向推导](#反向推导)           
   * [小结](#小结)                  
   * [实现](https://github.com/haiy/rush_in_dl/blob/master/rnn.py)

# RNN(Recurrent Neural Network)的前向和反向推导

## 前向计算

RNN的前向计算看起来非常简单，但是这里面比较重要的点在于理解$$s_t$$到底是什么。并且在这儿的$$t$$和RNN的隐含层大小
到底有木有关系。在这同样也采用交叉熵作为损失函数。需要注意的是因为整个前向计算其实是分成了t个步骤来算的，所以损失应该是这t步的损失
之和。虽然看起来比较简单，但是因为涉及到状态$$s_t$$的迭代，所以求导过程比NN稍微有些复杂。

在RNN中求导的变量是针则是分别U,V,W这三个权重变量的导数。因此目标偏导数主要是:

$$
\frac{\partial L}{\partial V},
\frac{\partial L}{\partial W},
 \frac{\partial L}{\partial U}
$$

经典的RNN展开计算过程:

<div id='rnnimg' align="center"><img src="/images/RNN.jpg" height="200px" align="middle"></div>

## forward

$$
s_t = tanh(Ux_t+Ws_{t-1}) \\
o_t = softmax(Vs_t) \\
L(y,o) = - \sum_t y_t log(o_t) = - \sum_t L_t
$$

## 反向推导

先作如下定义方便计算:

$$
q_t = Vs_t
$$

### 1. 计算$$\frac{\partial L_t}{\partial V}$$

$$
\begin{align}
\frac{\partial L_t}{\partial V} &= \frac{\partial L}{\partial q_t}\frac{\partial q_t}{\partial V} \\
&= (y- o_t)s_t
\end{align}
$$

### 2. 计算$$\frac{\partial L_t}{\partial W}$$

$$
\begin{align}
\frac{\partial L_t}{\partial W} &= \frac{\partial L}{\partial q_t}\frac{\partial q_t}{\partial s_t}
\frac{\partial s_t}{\partial W}
\\
&= (y- o_t)V \frac{\partial s_t}{\partial W}
\end{align}
$$

接下来的关键是$$\frac{\partial s_t}{\partial W}$$是$$s_t$$对W求导。但是因为

$$s_t = tanh(Ux_t + Ws_{t-1})$$

$$s_{t-1}$$也是W的函数,所以此时$$Ws_{t-1}$$求导类等价于形如$$f(x)g(x)$$的求导过程。

所以:

$$
\begin{align}

\frac{\partial s_t}{\partial W} &= \frac{\partial s_t}{\partial W} 
+ \frac{\partial s_t}{\partial s_{t-1}}\frac{\partial s_{t-1}}{\partial W} \\
&=\frac{\partial s_t}{\partial W} 
+ \frac{\partial s_t}{\partial s_{t-1}}(\frac{\partial s_{t-1}}{\partial W}) \\

&=\frac{\partial s_t}{\partial W} +   
\frac{\partial s_t}{\partial s_{t-1}} 
  ( \frac{\partial s_{t-1}}{\partial W}
    + \frac{\partial s_{t-1}}{\partial s_{t-2}} \frac{\partial s_{t-2}}{\partial W}
    ) \\

&=\frac{\partial s_t}{\partial W} + 
\frac{\partial s_t}{\partial s_{t-1}}    \frac{\partial s_{t-1}}{\partial W} +
\frac{\partial s_t}{\partial s_{t-1}}     
  \frac{\partial s_{t-1}}{\partial s_{t-2}} \frac{\partial s_{t-2}}{\partial W} \\

&=\frac{\partial s_t}{\partial W} + 
\frac{\partial s_t}{\partial s_{t-1}}    \frac{\partial s_{t-1}}{\partial W} +
\frac{\partial s_t}{\partial s_{t-2}}  \frac{\partial s_{t-2}}{\partial W} \\

&=\sum^t_{r=0}\frac{\partial s_t}{\partial s_r}\frac{\partial s_r}{\partial W}
\end{align}
$$ 

至此可求得:

$$
\frac{\partial L_t}{\partial W} = (y-o_t)V\sum^t_{r=0}\frac{\partial s_t}{\partial s_r}\frac{\partial s_r}{\partial W}
$$

可以明显看出其中关键是对$$\frac{\partial s_t}{\partial s_{t-1}}$$的求导:

$$
\begin{align}
\frac{\partial s_t}{\partial s_{t-1}} = (1 - s_{t}^2)W
\end{align}
$$

另： 

$$
\frac{\partial s_t'}{\partial W} = (1-s_t^2)s_{t-1}
$$


      
### 3. 计算$$\frac{\partial L_t}{\partial U}$$

U的计算过程同样包含了状态函数之间的依赖关系，所以其求导过程和W的基本一样，最后也可得到:

$$
\frac{\partial L_t}{\partial U} = (y-o_t)V\sum^t_{r=0}\frac{\partial s_t}{\partial s_r}\frac{\partial s_r}{\partial U}
$$

综合以上就是RNN的求导过程。

## 小结

对RNN的理解关键在于理解这个recurrent是怎么回事。其实网上很多图和文章都是抄来抄去，真正能帮人理解的东西却只有寥寥。
就目前来说，各种试图用所谓简化的方法来解释的文章都不如看懂公式来的直接。可能开始会比较难理解各个符号和表示，
可是这种精简正确的且非常有效的信息却是理解一点就学习一点的。

# 偏导推导Refs
  
   * <https://github.com/go2carter/nn-learn/blob/master/grad-deriv-tex/rnn-grad-deriv.pdf>   
   * <http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf>   
   * <http://songhuiming.github.io/pages/2017/08/20/build-recurrent-neural-network-from-scratch/>   
   * <https://gist.github.com/karpathy/d4dee566867f8291f086#file-min-char-rnn-py>   
   * <http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/>   
   * <http://colah.github.io/posts/2015-08-Understanding-LSTMs/>   
   * <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>   
   * <http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/>   
   * <https://stats.stackexchange.com/questions/222584/difference-between-feedback-rnn-and-lstm-gru?rq=1>   
   * <https://www.isca-speech.org/archive/archive_papers/interspeech_2010/i10_1045.pdf>    
   * <https://gist.github.com/karpathy/d4dee566867f8291f086>   
   * <http://suriyadeepan.github.io/2017-01-07-unfolding-rnn/>