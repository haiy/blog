---
title: 详解pyspark以及添加xgboost支持
layout: post
---

## {{page.title}}

<p class="meta">16 Jun 2018</p>


随着数据累积的不断增长，单机已经不能满足建模的性能需求。而xgb作为一个非常常用的模型，在spark支持上目前对java和scala的支持较好，但是没有pyspark的支持。鉴于使用成本，需要对pyspark进行调整和扩展支持xgb。

Table of Contents
=================
 * [1 技术难点](#1-技术难点)
 * [2 pyspark架构相关材料](#2-pyspark架构相关材料)
 * [3 关于性能的考量](#3-关于性能的考量)
 * [4 XGB中pyspark支持](#4-xgb中pyspark支持)
    * [4.1 认识py4j](#41-认识py4j)
    * [4.2 pyspark中的代码封装流程](#42-pyspark中的代码封装流程)
    * [4.3 pysparkWithXGBoost支持](#43-pysparkwithxgboost支持)
 * [5 pyspark模型上线](#5-pyspark模型上线)
 * [Refs](#refs)
    * [pyspark参考资料](#pyspark参考资料)
    * [pyspark性能对比参考资料](#pyspark性能对比参考资料)
    * [XGB项目中pyspark支持相关](#xgb项目中pyspark支持相关)
    * [pyspark模型上线相关](#pyspark模型上线相关)
    * [xgb on yarn](#xgb-on-yarn)
    * [gpu xgb](#gpu-xgb)
    * [不靠谱的xgb on yarn分布式](#不靠谱的xgb-on-yarn分布式)

### 1 技术难点

- pyspark是如何和原生的java代码进行交互的
- py模型的抽象架构
- xgb的java版如何嵌入
- 模型的保存和上线如何支持

### 2 pyspark架构相关材料

pyspark的实现总体来说是用py4j作为中间层将数据以二进制的方式在scala进程和python进程之间进行数据交换，在python进程内进行python函数的加工逻辑，在原始的spark框架内对二进制的数据进行RDD级别的操作。其主要流程如下：

- spark-submit/pyspark提交任务的时候首先判定是否是python app，调用PythonRunner，启动gateserver
- 启动sparkContext，和java executore，同时启动gateserver和python worker
- 任务执行时，数据以序列化byte方式读入，然后分发给不同python worker进行转换处理，处理好再以序列化方式放回jvm中进行数据的shuffle和落地

官方的架构图如下:

![](https://i.stack.imgur.com/sfcDU.jpg)


### 3 关于性能的考量

pyspark的性能损失主要在数据的序列化上，但是性能损失是否严重有待探讨。所以对性能提升主要可以从两个方面着手:

- 1 尽量采用原始的scala方法来处理数据，比如sql或者pyspark sql下的dsl，dataframe相关操作
- 2 采用单独的数据存储框架，这样python和java之间的数据转换成本就可以降低

综合看到的材料来说，pyspark其实性能相较于java版来说并无明显降低。可以酌情优化。


### 4 XGB中pyspark支持

首先不得不说的是官方的对python支持的重要性认识严重不足，这个是造成xgboost的pyspark不能很好支持的主要原因。其实通过现有封装代码的阅读，我们可以发现，其实对java版xgb的封装成本是非常小的。接下来就针对性的说下pyspark对算法部分的封装方法。

首先通览下[pyspark相关源码目录](https://gitee.com/arthurhu/spark/tree/master/python/pyspark)下的代码结构。主要有以下文件：

```bash
haiy@~/spark-1.6.1/python/pyspark$ ls
__init__.py         join.py             sql/
accumulators.py     ml/                 statcounter.py
broadcast.py        mllib/              status.py
cloudpickle.py      profiler.py         storagelevel.py
conf.py             rdd.py              streaming/
context.py          rddsampler.py       tests.py
daemon.py           resultiterable.py   traceback_utils.py
files.py            serializers.py      worker.py
heapq3.py           shell.py
java_gateway.py     shuffle.py
```

简单来说整个源码目录下的文件可以分为两类:

- 对现有java/scala包装部分，如`rdd.py`,`broadcast.py`
- pyspark特有的部分源码，如`java_gateway.py`,`cloudpickle.py`


那么接下来从哪儿开始呢?分三步即可，

- 1 [认识py4j,弄清楚py4j到底是如何调用java的](#认识py4j)
- 2 [pyspark框架如何调用raw spark，也即是scala版的spark的](#pyspark中的代码封装流程)
- 3 [pyspark的ml部分是xgb如何封装代码的](#pyspark-with-XGBoost)

#### 4.1 认识py4j

[py4j](https://www.py4j.org/contents.html),'A Bridge between Python and Java'。其slogan简洁明了。话不多说，show me code。按照[教程](https://www.py4j.org/getting_started.html)来,详细的代码参照[这个](https://github.com/haiy/test_project/tree/master/py4j):

- 1 环境准备
    先通过`pip install py4j`安装py4j, 然后[下载这个py4j的jar](https://github.com/haiy/test_project/raw/master/py4j/py4j-0.10.1.jar)

- 2 show you code

    java端：
    
    ```java
    import py4j.GatewayServer;
    
    public class MySimpleServer{ 
        public static void main(String[] args) {
            GatewayServer gatewayServer = new GatewayServer(new MySimpleServer());
            gatewayServer.start();
            System.out.println("Gateway Server Started");
        }
    }
    ```
    
    然后编译启动：

    ```bash
    echo "ready to compile ..."
    rm *.class
    javac -cp .:py4j-0.10.1.jar MySimpleServer.java
    echo "begin to start gateserver..."
    java -cp .:py4j-0.10.1.jar MySimpleServer
    ```
    
    python端，玩耍吧：
    
    ```python
    
    from py4j.java_gateway import JavaGateway
    
    # build a bridge, what happend if kill the server now
     gw = JavaGateway()
     s = gw.entry_point.getStack()
     s.pop()
    
     s.push("hi")
    
    # try to crash it
     s.pop()
     s.pop
     
    In [31]: java_list = gw.jvm.java.util.LinkedList()
    
    In [32]: java_list.add("hasda")
    Out[32]: True
    
    In [33]: java_list
    Out[33]: ['hasda']
    
    In [34]: java_list 
    ```

#### 4.2 pyspark中的代码封装流程

- 1 play with the sparkContext

    本地启动pyspark, 用ipython做为driver，`export PYSPARK_DRIVER_PYTHON=ipython`
    
    ```python
    # 输入spark.jsc然后tab可以看看支持的方法，有jvm，_jsc也就是java sparkContext
    In [1]: spark._jsc.                                                            
                                                                                                                                                                                   
    # 先导入两个模型训练数据需要的基本结构
    In [34]: from pyspark.ml.linalg import Vectors
    
    In [35]: from pyspark.sql.types import Row
    
    # 真正的magic。直接call一个java的定义
    In [36]: lr = spark.sparkContext._jvm.org.apache.spark.ml.classification.LogisticRegression()
    
    # 来创建个训练的df
    In [37]: train_df = spark.createDataFrame( [ Row(label=1,features=Vectors.dense([1,2,3])), Row(label=0,fe
        ...: atures=Vectors.dense([4,5,6])) ]) 
    
    In [38]: train_df
    Out[38]: DataFrame[features: vector, label: bigint]
    
    # 来训练下模型
    In [39]: lr_model = lr.fit(train_df._jdf)
    
    ```

- 2 源码跟读

    以pyspark中的[LogisticRegression](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/classification.html#LogisticRegression)为例来简单分析。
    其[类初始化方法](https://github.com/apache/spark/blob/c7c0b086a0b18424725433ade840d5121ac2b86e/python/pyspark/ml/classification.py#L309)主要就这么几行:
    
    ```python
    super(LogisticRegression, self).__init__()
            self._java_obj = self._new_java_obj(
                "org.apache.spark.ml.classification.LogisticRegression", self.uid)
            self._setDefault(maxIter=100, regParam=0.0, tol=1E-6, threshold=0.5, family="auto")
            kwargs = self._input_kwargs
            self.setParams(**kwargs)
            self._checkThresholdConsistency()
    ```
    
    其中可以看到关键是用`_new_java_obj`调用java中的方法创建真正的类对象。那么这个方法从哪儿来的呢？根据其继承关系，我们可以找到其所在的[JavaWrapper](https://github.com/apache/spark/blob/c7c0b086a0b18424725433ade840d5121ac2b86e/python/pyspark/ml/wrapper.py#L31)，
    可以看到这个类封装了很多的python调用java的方法。整体来说还是比较简单的。

- 3 `pipeline model`和`ParaGridBuilder`支持

    在pyspark中，我们可以看到在[最新的api文档](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html)中比较明确了一个pipeline的概念抽象，这个概念和我们在sklearn中以及我们自己的建模流程中是非常一致的。那么如何让我们的xgb也能直接集成在pyspark的piepline中呢？
    
    此外，为了高效的建模，我们通常也会用一些超参空间搜索的策略，在spark中支持的就是用`ParamGridBuilder`来进行GridSearch，那么如何才能使xgb也能加入进来呢？
    
    为了解决以上两个问题，需要的事情其实一样，就是弄清，我们的pipeline和Paragridbuilder是怎么起作用的？
    先说pipeline,从[其源码实现](https://github.com/apache/spark/blob/c7c0b086a0b18424725433ade840d5121ac2b86e/python/pyspark/ml/pipeline.py#L109) 可以看到其在训练过程不过是直接调用了每个stage的transform或者fit方法。在保存的时候直接将训练好的模型对应信息存进文件，主要存储的信息有:

    - 类名
    - 参数信息
    - stage名字

    那么PipelineModel是如何加载的呢？先不看代码我们自己可以想下，如果是我们来实现这个pipeline的话会如何来加载保存的模型？
    
    **pyspark到底是怎么做的呢？**
    
    通过其[源码](https://github.com/apache/spark/blob/c7c0b086a0b18424725433ade840d5121ac2b86e/python/pyspark/ml/pipeline.py#L244)深入
    跟进可以发现其模型文件的load主要分两种一种是直接调用[JavaMLReader](https://github.com/apache/spark/blob/c7c0b086a0b18424725433ade840d5121ac2b86e/python/pyspark/ml/util.py#L250)，另一种是调用[DefaultParamsReader.loadParamsInstance](https://github.com/apache/spark/blob/c7c0b086a0b18424725433ade840d5121ac2b86e/python/pyspark/ml/util.py#L558)。
    
    根本来说，这两者的基本思路都是根据meta中定义的类来创建对应的对象，然后设置类参数，从而实现对模型的还原。只不过py模型多了一个python类名和java
    类名之间的映射关系。那么至此其实如何支持pipeline已经没难点了。
    
    至于Paragridbuilder,我们需要想清楚的是其调用方式是什么样的？其实简单跟一下代码也就知道怎么回事了。

#### 4.3 pysparkWithXGBoost支持
   
- 1 在任何能起作用PYTHONPATH下 `mkdir -p ml/dmlc/xgboost4j/scala`

- 2 将下面代码copy到 `ml/dmlc/xgboost4j/scala/spark.py`
        
    ```python
        
    from pyspark.ml.classification import JavaClassificationModel, JavaMLWritable, JavaMLReadable, TypeConverters, Param, \
        Params, HasFeaturesCol, HasLabelCol, HasPredictionCol, HasRawPredictionCol, SparkContext
    from pyspark.ml.wrapper import JavaModel, JavaWrapper, JavaEstimator
    
    
    class XGBParams(Params):
        '''
    
        '''
        eta = Param(Params._dummy(), "eta",
                    "step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features. and eta actually shrinks the feature weights to make the boosting process more conservative",
                    typeConverter=TypeConverters.toFloat)
        max_depth = Param(Params._dummy(), "max_depth",
                          "maximum depth of a tree, increase this value will make the model more complex / likely to be overfitting. 0 indicates no limit, limit is required for depth-wise grow policy.range: [0,∞]",
                          typeConverter=TypeConverters.toInt)
        min_child_weight = Param(Params._dummy(), "min_child_weight",
                                 "minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will berange: [0,∞]",
                                 typeConverter=TypeConverters.toFloat)
        max_delta_step = Param(Params._dummy(), "max_delta_step",
                               "Maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update.",
                               typeConverter=TypeConverters.toInt)
        subsample = Param(Params._dummy(), "subsample",
                          "subsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting.",
                          typeConverter=TypeConverters.toFloat)
        colsample_bytree = Param(Params._dummy(), "colsample_bytree",
                                 "subsample ratio of columns when constructing each tree",
                                 typeConverter=TypeConverters.toFloat)
        colsample_bylevel = Param(Params._dummy(), "colsample_bylevel",
                                  "subsample ratio of columns for each split, in each level.",
                                  typeConverter=TypeConverters.toFloat)
        max_leaves = Param(Params._dummy(), "max_leaves",
                           "Maximum number of nodes to be added. Only relevant for the ‘lossguide’ grow policy.",
                           typeConverter=TypeConverters.toInt)
    
        def __init__(self):
            super(XGBParams, self).__init__()
    
    class XGBoostClassifier(JavaEstimator, JavaMLWritable, JavaMLReadable, XGBParams,
                            HasFeaturesCol, HasLabelCol, HasPredictionCol, HasRawPredictionCol):
        def __init__(self, paramMap={}):
            super(XGBoostClassifier, self).__init__()
            scalaMap = SparkContext._active_spark_context._jvm.PythonUtils.toScalaMap(paramMap)
            self._java_obj = self._new_java_obj("ml.dmlc.xgboost4j.scala.spark.XGBoostEstimator", self.uid, scalaMap)
            self._defaultParamMap = paramMap
            self._paramMap = paramMap
    
        def setParams(self, paramMap={}):
            return self._set(paramMap)
    
        def _create_model(self, java_model):
            return XGBoostClassificationModel(java_model)
    
    
    class XGBoostClassificationModel(JavaModel, JavaClassificationModel, JavaMLWritable, JavaMLReadable):
    
        def getBooster(self):
            return self._call_java("booster")
    
        def saveBooster(self, save_path):
            jxgb = JavaWrapper(self.getBooster())
            jxgb._call_java("saveModel", save_path)
    
    ```
    
- 3 play it as a normal pyspark model!


### 5 pyspark模型上线

上面讲了pyspark和xgb的训练过程，那么对于企业场景来说，仅仅考虑这些还是不够的，我们还要考虑这个玩意怎么上线？比较navie的方式是直接用pyspark
来导出pipeline model，然后serving。经过评测，这种方式实际的预测耗时在100ms～。对于一些要求较高的场景不够的。那么该如何解决呢？

前面我们大概知道了spark中的pipeline是如何加载模型的，那么在[scala源码中](https://github.com/apache/spark/blob/c7c0b086a0b18424725433ade840d5121ac2b86e/mllib/src/main/scala/org/apache/spark/ml/util/ReadWrite.scala#L649)其加载方式其实也是一致的。

那么耗时的地方在哪儿呢？从模型的预测流程来说，我们首先要创建sparkContext，然后将数据转换成spark能够支持的数据结构DataFrame，dense Vector,
预测后数据还要转成单机的传统数据结构，通常以json形式输出。而在模型预测里真正对我们有用的关键信息是model的参数信息。

所以如果需要优化的话那么根本的优化点是将数据环境从分布式转换成单机的，也就是剔掉对sparkContext的依赖，将预测的数据结构直接做成单机的。

spark官方的砖厂已经做了这个支持。开源方案内比较好用的是[mleap](https://github.com/combust/mleap),其文档中的性能对比测试已经做到微秒级还是很promising的。

### Refs

#### pyspark参考资料

  - 比较老的[架构设计方案](https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals)
  - [稍微详细点的](https://stackoverflow.com/questions/30684982/how-does-spark-interoperate-with-cpython)
  - [spark submit启动py4j作为python app的starter](https://github.com/apache/spark/blob/3e5b4ae63a468858ff8b9f7f3231cc877846a0af/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala#L57)
  - [py spark app start](https://github.com/apache/spark/tree/master/core/src/main/scala/org/apache/spark/api/python)

#### pyspark性能对比参考资料  

- [如何高效的使用pyspark](https://stackoverflow.com/questions/31684842/calling-java-scala-function-from-a-task)
- [有可能有偏差的对比](https://stackoverflow.com/questions/32464122/spark-performance-for-scala-vs-python/32471016#32471016)
- [dive into pyspark](https://www.slideshare.net/mateuszbuskiewicz/dive-into-pyspark?qid=f6badd4a-7094-4b6d-8d50-c94c5a1171a5&v=&b=&from_search=7)

#### XGB项目中pyspark支持相关

- [XGB spark src](https://github.com/dmlc/xgboost/blob/master/jvm-packages/xgboost4j-spark/src/main/scala/ml/dmlc/xgboost4j/scala/spark/XGBoostEstimator.scala)
- [basic pyspark wrapper](https://github.com/dmlc/xgboost/issues/1698#issuecomment-391304315)

#### pyspark模型上线相关

- [ibm spark ml flow](https://dataplatform.ibm.com/analytics/notebooks/c8652d2c-bfc9-4354-8168-f1c9f7f8dfc2/view?access_token=02a83fea8450a452c8de76af98dae078459d0f56810ddef4f4c62d5bc4fc72cf)
- [PMML openscoring](https://github.com/openscoring/openscoring)
- [hydro-serving](https://github.com/Hydrospheredata/hydro-serving)
- [hydro-serving-slides](https://www.slideshare.net/StepanPushkarev/spark-ml-pipeline-serving)
- [mleap](https://github.com/combust/mleap)
- [dbml-local](https://github.com/databricks/databricks-ml-examples/tree/master/model-export-demo)
- [dbml local api doc](https://databricks.github.io/databricks-ml/latest/)
- [model-import doc](https://docs.azuredatabricks.net/spark/latest/mllib/model-import.html)
- [pyspark model export with mleap](https://docs.databricks.com/spark/latest/mllib/mleap-model-export.html#export-and-import-models-in-scala)
- [mleap official export](https://github.com/combust/mleap-demo/blob/master/notebooks/PySpark%20-%20AirBnb.ipynb)
- [mleap pyspark](http://mleap-docs.combust.ml/getting-started/py-spark.html)

#### xgb on yarn

- [official doc](https://github.com/dmlc/xgboost/tree/master/demo/distributed-training)
- [aws yarn build tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/aws_yarn.html)
- [部署](https://blog.csdn.net/u010306433/article/details/51403894)
- [部署2](https://blog.csdn.net/webzjuyujun/article/details/78553518)
- [pyspark vs sagemaker](https://medium.com/@julsimon/building-a-spam-classifier-pyspark-mllib-vs-sagemaker-xgboost-1980158a900f)
- [dist py xgb](https://github.com/dmlc/xgboost/pull/897)
- [dist xgb py example](https://github.com/tqchen/xgboost/blob/master/tests/distributed/test_basic.py)
- [dist xgb](http://wormhole.readthedocs.io/en/latest/index.html)
- [这个靠谱点的pyspark](https://github.com/dmlc/xgboost/issues/1698#issuecomment-391304315)

#### gpu xgb

需要搞清楚的是为什么GPU可以加速，基于GPU的xgb提速还是非常明显的

- [GPU-accelerated-xgboost](http://dmlc.ml/2016/12/14/GPU-accelerated-xgboost.html)
- [XGBoost GPU Support](https://xgboost.readthedocs.io/en/latest/gpu/index.html)
- [gradient-boosting-decision-trees-xgboost-cuda/](https://devblogs.nvidia.com/gradient-boosting-decision-trees-xgboost-cuda/)

#### 不靠谱的xgb on yarn分布式

```bash
TASK_DIR=`pwd`
git clone --recursive https://github.com/dmlc/xgboost
git checkout v0.60
mkdir xgboost-package
cp -r xgboost xgboost-packages/
wget "http://ftp.gnu.org/gnu/gcc/gcc-4.8.2/gcc-4.8.2.tar.bz2"
tar -jxvf gcc-4.8.2.tar.bz2
cd gcc-4.8.2
mkdir ./contrib/download_prerequisites
cd ..
mkdir gcc-build-4.8.2
cd  gcc-build-4.8.2
../gcc-4.8.2/configure --enable-checking=release --enable-languages=c,c++ --disable-multilib --prefix=$TASK_DIR

make -j10
make install
export PATH=$TASK_DIR/bin:$PATH
cp -r $TASK_DIR/lib64 $TASK_DIR/xgboost-packages

wget "https://cmake.org/files/v3.5/cmake-3.5.2.tar.gz"
tar -zxf cmake-3.5.2.tar.gz
cd cmake-3.5.2
./bootstrap --prefix=${TASK_DIR}
gmake
make -j10
make install
```
