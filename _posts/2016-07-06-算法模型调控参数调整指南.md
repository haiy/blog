---
title: 算法模型调控参数调整指南
date: 2016-07-06 00:00:00 Z
layout: post
---

{{page.title}}
=====
<p class="meta">July 06 2016</p>

主要整理汇总了常用的三个算法模型LR，SVM，XGBoost涉及到的参数定义及其对模型的影响效果。其中XGBoost的参数涉及到了Decision Tree，Random Froest，Gradient Boost Decision Tree这四种模型的参数。模型训练调控参数也就是hyperparameter。对这些参数的调整主要影响模型的训练速度，表示能力，泛化能力。

**Logistic Regression模型调节参数说明**

对于逻辑回归模型来说，最重要的模型调节参数就是正则项的参数。正则项的参数是对训练出的模型中各个特征权重进行运算得到的。LR的正则主要有L1和L2。L1和L2都是避免数据过拟合的方法。举例来说二者计算方式的差异，
有两个向量，a=(0.5,0.5), b=(-1,0)，其L1和L2范数分别为:

```
L1范数： ||a||1 = |0.5|+|0.5|=1 ||b||1 = |-1|+|0.5|=1
L2范数： ||a||2 =sqrt( 0.5^2+0.5^2)=1/sqrt(2)<1  ||b||2 = sqrt((-1)^2+0^2)=1
```
那么采用L1的计算方法的时候二者的L1范数都相等，但是二者的L2范数却表现出来是a比b的L2范数更小，因此L2的思想其实是把结果分散到其他的。
L1 范数是系数的绝对值和，L2范数是系数的平方和开平方。

**XGBoost模型调节参数说明**

三类参数：一般参数，booster参数，任务参数

- 1 eta 默认 0.3 [0,1]
    - 每步的缩小步长来避免过拟合。每个boosting步骤都会直接得新的特征权重，eta就是用来收缩特征权重
- 2 gamma 默认0   [0,∞]
    - 在树的叶子节点进行分割所需要的最小损失减少值，值越大，算法也就越保守
- 3 max_depth  最大深度 6   [0,∞]
    - 单棵树的最大深度，越深树模型越复杂，也容易过拟合
- 4  min_child_weight 1
    - 子节点实例最小权重和。对于树的分割来说，如果叶子节点的最小权重和小于该值，那么建树过程就会停止对该节点
的进一步分割。在线性回归模型 这个参数就直接对应于每个节点的最小实例数。值越大模型越保守。
- 5 max_delta_step  default=0
    - 每棵树权重最大允许值。设置为0的话是不设限制。如果设置是个正值，则会使更新步骤更保守。通常不需要设置该参数，但是当在用逻辑回归处理.十分不平衡的数据集的时候，将该参数设置在1~10之间能帮助控制数据的更新
- 6  subsample [default=1] 子样本 (0,1]
    - 训练集的子样本比例。设置成0.5的时候就意味着XGBoost会随机的从数据集合中抽取一般来进行树的增长，这样也能避免过拟合
- 7  colsample_bytree [default=1] range: (0,1]
    - 每棵树的构建采用多少列的比例
- 8  colsample_bylevel [default=1]
    - 每个分割点的子集比例
- 9 lambda[default=1]
    - L2正则项
- 10 alpha L1正则项
- 11 tree_method, string [default='auto']
    - 建树的算法。字符串，选项，{‘auto’,'exact','approx'}
    - 'auto'  启发式选中最快的模式
    - 对中小数据集采用严格的贪婪算法
    - 对很大的数据集，采用近似算法

**树模型通用参数**

- 1 树的深度。越深越容易过拟合，通常的深度约为特征数的平方根
- 2 树的个数
- 3 单棵树的特征采样比
- 4 叶子节点最小样本数

**模型复杂度控制参数**

max_depth, min_child_weight,gamma

**抗噪**

subsample, colsample_bytree

如果模型过拟合，就减小步长，增加迭代轮数

**SVM模型训练调节参数**

- 1 C 惩罚因子 软间隔参数
    - 正则化的方法
    - 太大就是对不可分点的惩罚力度更大，可能会得到更多的支持向量，模型也容易过拟合；太小，容易欠拟合
    - 控制模型决策条件的复杂度和错误频度
    - 参数C控制了SVM在训练集上优化过程中避免误分的程度，C越大平面的边界越小
    - 选择范围广泛点，10^[-5,...,5]
- 2 epsilon
    - 样本中支持向量的比例
    - 要设置最优的spsilon，需要对数据中的噪声水平了解清楚
    - e增大意味着降低对模型的准确度要求，也会降低支持向量的个数
    - e代表着我们对近似函数的准确度要求水平，e值大于目标值，那么结果不会好，但是如果e为0，那么又会过拟合，通常选小一点
    - e主要是控制的是对训练数据的支持向量多少的程度，通常为50%
- 3 核函数参数
    - gamma是高斯核函数和多项式核函数的参数
    - libsvm gamma 默认值 1/特征数
    - 设置具体方法要用交叉验证来确定
    - 控制着SVM对训练集的灵活度，越大模型越“弯”也容易过拟合，越小模型越“直”，

**如何选择核函数？**

主要是根据数据来确定

- 1 先试试线性核函数
- 2 然后试试非线性核函数是否能提高效果
- 3 高斯核和多项式之间，经验表明面对标准化后的数通常高斯核较好

**normalization**

- 1 线性模型模型对数据尺度还是很敏感的
- 2 通常比较有益的做法是，对每个特征值减去均值除以标准差
- 3 但对稀疏类型数据，这种方法破坏数据稀疏性，这时候可以用分箱的处理方法

**参考：**

- [kaggle.tune-rf-parameters](https://www.kaggle.com/forums/f/15/kaggle-forum/t/4092/how-to-tune-rf-parameters-in-practice)  
- [quora.L1&L2](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization/answer/Justin-Solomon)     
- [SVM.parameters](http://www.svms.org/parameters/)   
- [SVM.C](http://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel)   
- [how to tune](http://www.cs.colostate.edu/~asa/pdfs/howto.pdf)  
- [xgboost parameter](http://www.slideshare.net/ShangxuanZhang/kaggle-winning-solution-xgboost-algorithm-let-us-learn-from-its-author)  
